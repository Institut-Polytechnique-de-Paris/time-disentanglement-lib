<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Primary Meta Tags -->
    <title>Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference</title>
    <meta name="title" content="Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference">
    <meta name="description"
        content="Learning disentangled representations for time series is a promising path to facilitate reliable generalization to in- and out-of distribution (OOD), offering benefits like feature derivation and improved interpretability and fairness, thereby enhancing downstream tasks. We focus on disentangled representation learning for home appliance electricity usage, enabling users to understand and optimize their consumption for a reduced carbon footprint. Our approach frames the problem as disentangling each attribute's role in total consumption. Unlike existing methods assuming attribute independence which leads to non-identiability, we acknowledge real-world time series attribute correlations, learned up to a smooth bijection using contrastive learning and a single autoencoder. To address this, we propose a Disentanglement under Independence-Of-Support via Contrastive Learning (DIOSC), facilitating representation generalization across diverse correlated scenarios. Our method utilizes innovative \textit{l}-variational inference layers with self-attention, effectively addressing temporal dependencies across bottom-up and top-down networks. We find that DIOSC can enhance the task of representation of time series electricity consumption. We introduce TDS (Time Disentangling Score) to gauge disentanglement quality. TDS reliably reflects disentanglement performance, making it a valuable metric for evaluating time series representations disentanglement."
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://brendel-group.github.io/cl-ica/">
    <meta property="og:title" content="Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference">
    <meta property="og:description"
        content="Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.">
    <meta property="og:image" content="https://brendel-group.github.io/cl-ica/img/overview_compressed.svg">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://brendel-group.github.io/cl-ica/">
    <meta property="twitter:title" content="Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference">
    <meta property="twitter:description"
        content="Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.">
    <meta property="twitter:image" content="https://brendel-group.github.io/cl-ica/img/overview_compressed.svg">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
        integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>

    <style>
        .main {
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        .code {
            font-family: 'IBM Plex Mono', monospace;
        }

        .row {
            padding-bottom: 20px;
        }

        .a {
            color: gainsboro;
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        td {
            padding: 0 15px;
        }

        p {
            text-align: justify;

        }
    </style>
      <style>
    h2 {
      color: #3174ad;
      text-align: center;
    }

    p {
      margin: 15px 0;
      color: #555;
    }

    .dataset, .scenario {
      background-color: #eaf3fc;
      border-left: 4px solid #3174ad;
      padding: 10px;
      margin: 10px 0;
      border-radius: 5px;
    }

    .appliance-list, .scenario-list {
      margin-top: 5px;
      color: #3174ad;
    }

    .highlight {
      font-weight: bold;
      color: #3174ad;
    }
  </style>

    <title>Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference</title>
</head>

<body>
    <div style="display: none;">
        \[

        \renewcommand{\d}{{\bf{d}}}
        \renewcommand{\b}{{\bf{b}}}
        \newcommand{\J}{{\bf{J}}}
        \newcommand{\A}{\bf{A}}
        \newcommand{\B}{\bf{B}}
        \newcommand{\RR}{\mathbf{R}}
        \newcommand{\h}{{\bf{h}}}
        \newcommand{\x}{{\bf{x}}}
        \newcommand{\bfa}{{\bf{a}}}
        \newcommand{\bfb}{{\bf{b}}}
        \newcommand{\bfc}{{\bf{c}}}
        \newcommand{\y}{{\bf{y}}}
        \newcommand{\z}{{\bf{z}}}
        \newcommand{\w}{{\bf{w}}}
        \newcommand{\f}{{\bf{f}}}
        \newcommand{\tf}{{\bf{\tilde f}}}
        \newcommand{\tx}{{\bf{\tilde x}}}
        \renewcommand{\d}{{\rm{d}}}
        \newcommand{\s}{{\bf{s}}}
        \newcommand{\g}{{\bf{g}}}
        \newcommand{\W}{{\bf{W}}}
        \newcommand{\vol}{{\operatorname{vol}}}
        \newcommand{\zz}{\mathbf{z}}
        \newcommand{\xx}{\mathbf{x}}
        \newcommand{\bdelta}{\bm{\delta}}
        \renewcommand{\H}{\mathbf{H}}
        \newcommand{\txx}{{\tilde{\mathbf{x}}}}
        \newcommand{\tzz}{{\tilde{\mathbf{z}}}}
        \newcommand{\tyy}{{\tilde{\mathbf{y}}}}
        \newcommand{\invf}{f^{-1}}
        \newcommand{\Sp}{\mathbb{S}}
        \]
    </div>

    <div class="container main">
        <div class="row">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8" id="main-content">
                <div class="row text-center my-5" id="#">
                    <h1>Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference</h1>
                </div>

                <!-- Begin author list-->
                <div class="row text-center mb-4">
                    <div class="col-sm-4  mb-4">
                        Khalid Oublal*
                        <a href="mailto:oublalkhalid@gmail.com"><i class="far fa-envelope"></i></a>
                        <a href="https://oublalkhalid.com" target="_blank"><i class="fas fa-link"></i></a></br> 
                        Institute Polytechnique de Paris
                    </div>
                    <div class="col-sm-4 mb-4">
                         David Benhaiem
                        <a href="https://scholar.google.fr/citations?hl=fr&user=IL-2GPoAAAAJ&view_op=list_works&sortby=pubdate"><i class="far fa-envelope"></i></a>
                        <a href="https://scholar.google.fr/citations?hl=fr&user=IL-2GPoAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><i class="fas fa-link"></i></a></br>
                        OneTech TotalEnergies & <nobr>DS-AI</nobr>
                    </div>
                    <div class="col-sm-4  mb-4">
                        SaÃ¯d Ladjal
                        <a href="https://perso.telecom-paristech.fr/ladjal/cv.html"><i class="far fa-envelope"></i></a>
                        <a href="https://perso.telecom-paristech.fr/ladjal/cv.html" target="_blank"><i class="fas fa-link"></i></a></br> 
                        Institute Polytechnique de Paris, Telecom Paris LTCI/S2A
                    </div>
                    <div class="col-sm-4 mb-4">
                        Emmanuel Le-Borg
                        <a href="#"><i class="far fa-envelope"></i></a>
                        <a href="#" target="_blank"><i class="fas fa-link"></i></a></br>
                        OneTech TotalEnergies & <nobr>DS-AI</nobr>
                    </div>
                    <div class="col-sm-2  mb-2">
                    </div>
                    <div class="col-sm-4  mb-4">
                        FranÃ§ois Roueff
                        <a href="#" target="_blank"><i class="fas fa-link"></i></a><br>
                        Institute Polytechnique de Paris, Telecom Paris LTCI/S2A
                    </div>
                </div>
                <!-- End author list-->

                <div class="row text-center">
                    <div class="col-sm-4 mb-4">
                        <h4>
                            <a href="https://openreview.net/pdf?id=iI7hZSczxE" target="_blank">
                                <i class="fas fa-file-alt"></i>
                                Paper
                            </a>
                        </h4>
                    </div>
                    <div class="col-sm-4 mb-4">
                        <h4>
                            <a href="https://openreview.net/pdf?id=iI7hZSczxE" target="_blank">
                                <i class="far fa-chart-bar"></i>
                                Dataset
                            </a>
                        </h4>
                    </div>
                    <div class="col-sm-4 mb-4">
                        <h4>
                            <a href="https://github.com/Institut-Polytechnique-de-Paris/time-disentanglement-lib" target="_blank"> <i
                                    class="fab fa-github"></i>
                                Code
                            </a>
                        </h4>
                    </div>
                </div>

                <div class="row text-center">
                    <p>
                        <b>tl;dr:</b>
                        <span class="text-muted">
                            This work disentangles factors in time series data, allowing better understanding and optimization. 
                            It handles real-world correlations between factors and introduces TDS (Time Disentangling Score) to measure disentanglement quality effectively.
                        </span>
                    </p>
                </div>

                <div class="row mt-2">
                    <h3>News</h3>
                </div>

                <div class="row">
                    <table>
                        <tr>
                            <td class="mr-10">
                                <span class="badge badge-pill badge-primary">May '21</span>
                            </td>
                            <td>
                                The paper was accepted at ICLR 2024 ðŸš€ðŸš€!
                            </td>
                        </tr>
                        <tr>
                            <td class="mr-10">
                                <span class="badge badge-pill badge-primary">February '21</span>
                            </td>
                            <td>
                                The pre-print is now available on arXiv: <a
                                    href="https://openreview.net/pdf?id=iI7hZSczxE" target="_blank">arxiv.org/abs/2102.08850</a>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">December '20</span>
                            </td>
                            <td>
                                A shorter <a
                                    href="https://sslneuips20.github.io/files/CameraReadys%203-77/67/CameraReady/Contrastive_Learning_can_Identify_the_Underlying_Generative_Factors_of_the_Data_SSL_NeurIPS_2020.pdf"><i
                                        class="far fa-sticky-note"></i> workshop version</a> of the paper was accepted
                                for spotlight presentation at the <a href="https://unireps.org/2023/"
                                    target="_blank">NeurIPS 2023 Workshop on Unifying Representations in Neural Models</a>.
                            </td>
                        </tr>
                    </table>
                </div>

                <div class="row mt-2">
                    <h3>Abstract</h3>
                </div>
                <div class="row">
                    <p>
                        Learning disentangled representations is crucial for Time Series, offering benefits like feature derivation and improved interpretability, thereby enhancing task
                          performance. We focus on disentangled representation learning for home appliance electricity usage, enabling users to understand and optimize their consumption for a reduced carbon footprint. Our approach frames the problem as disentangling each attributeâ€™s role in total consumption (e.g., dishwashers, fridges,
                          etc). Unlike existing methods assuming attribute independence, we acknowledge
                          real-world time series attribute correlations, like the operating of dishwashers and
                          washing machines during the winter season. To tackle this, we employ weakly
                          supervised contrastive disentanglement, facilitating representation generalization
                          across diverse correlated scenarios and new households. Our method utilizes innovative l-variational inference layers with self-attention, effectively addressing
                          temporal dependencies across bottom-up and top-down networks. We find that
                          DIoSC (Disentanglement and Independence-of-Support via Contrastive Learning) can enhance the task of reconstructing electricity consumption for individual
                          appliances. We introduce TDS (Time Disentangling Score) to gauge disentanglement quality. TDS reliably reflects disentanglement performance, making it
                          a valuable metric for evaluating time series representations.
                    </p>
                </div>

                <div class="row mt-2">
                    <div class="col-12">
                        <img src="static/images/model.png" style="width: 100%;" />
                        <small class="text-muted">
                            Overview: We focus on disentangled representation learning for home appliance electricity usage, enabling users to understand and optimize their consumption for a reduced carbon footprint. Our approach frames the problem as disentangling each attributeâ€™s role in total consumption (e.g., dishwashers, fridges,
                          etc). Unlike existing methods assuming attribute independence, we acknowledge
                          real-world time series attribute correlations, like the operating of dishwashers and
                          washing machines during the winter season. We assume
                            the observations are generated by an (unknown) injective generative model \(g\) that maps
                            unobservable latent variables from a hypersphere to observations in another manifold. Under
                            these assumptions, the feature encoder \(f\) implictly learns to invert the ground-truth
                            generative process \(g\) up to linear transformations, i.e., \(f = \mathbf{A} g^{-1}\) with
                            an orthogonal matrix \(\mathbf{A}\), if \(f\) minimizes the InfoNCE objective.
                        </small>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Contributions</h3>
                </div>
                <ol>
                    <li>
                        We establish a theoretical connection between the InfoNCE family of objectives, which is
                        commonly used in self-supervised learning, and nonlinear ICA. We show that training with InfoNCE
                        inverts the data-generating process if certain statistical assumptions on the data generating
                        process hold.
                    </li>
                    <li>
                        We empirically verify our predictions when the assumed theoretical conditions are fulfilled. In
                        addition, we show a successful inversion of the data-generating process even if theoretical
                        assumptions are partially violated.
                    </li>
                    <li>
                        We build on top of the CLEVR rendering pipeline (Johnson et al., 2017) to generate a more
                        visually complex disentanglement benchmark, called <i>3DIdent</i>, that contains hallmarks of
                        natural environments (shadows, different lighting conditions, a 3D object, etc.). We demonstrate
                        that a contrastive loss derived from our theoretical framework can identify the ground-truth
                        factors of such complex, high-resolution images.
                    </li>
                </ol>


                <div class="row mt-2">
                    <h3>Theory</h3>
                </div>

                <div class="row mt-2">
                    <div class="col-12">
                        <p>
                            To achieve disentanglement, invariant, and aligned latent representations, we use a contrastive objective that ensures each latent component \(z_m\) is only influenced by its corresponding output in the decoder. Specifically, disentanglement is achieved when each ground truth variable \(y_m\) aligns one-to-one with \(z_m\), even with limited labels in settings like NILM (Non-Intrusive Load Monitoring).
                        </p>
                        
                        <p>
                            We build on weakly supervised contrastive learning, using an objective modified from Zbontar et al. (2021). This objective enforces two core components:
                            <ul>
                                <li><strong>Latent-Invariant</strong>: Minimizes overlap between \(z_m\) and its negatives \(z_m^{-}\), reducing redundant information.</li>
                                <li><strong>Latent-Alignment</strong>: Encourages similarity between \(z_m\) and its augmented \(z_m^{+}\), ensuring variations align with changes in ground truth attributes.</li>
                            </ul>
                        </p>
                
                        <p>
                            We empirically relax independence assumptions (Assumption 4.1) by enforcing independence of support (IoS) in mini-batches, ensuring that augmented latents \(Z_{:,m}^+\) are close to their original \(Z_{:,m}\) and far from negatives \(Z_{:,m}^-\), without requiring Cartesian product support.
                        </p>
                
                        <p>
                            The final objective combines these components:
                            \[
                            L_{\text{DIOSC}} = \eta \sum_m \sum_V D(z_m, z_m^{-})^2 \, |_{\text{Latent-Invariant}} + \sum_m \sum_U \left(1 - D(z_m, z_m^{+})\right)^2 \, |_{\text{Latent-Alignment}},
                            \]
                            where \(D(\cdot, \cdot)\) denotes cosine similarity distance, and \(\eta = 1\).
                        </p>
                    </div>
                </div>

                  <div class="container">
    <h2>Datasets</h2>
    <p>We conducted experiments on three public datasets:</p>
    
    <div class="dataset">
      <p><span class="highlight">UK-DALE</span> (Kelly & Knottenbelt, 2015),</p>
      <p><span class="highlight">REDD</span> (Kolter & Johnson, 2011),</p>
      <p><span class="highlight">REFIT</span> (Murray et al., 2017)</p>
      <p>These datasets provide power measurements from multiple homes.</p>
    </div>

    <p>We focus on six appliances:</p>
    <ul class="appliance-list">
      <li>Washing Machine</li>
      <li>Oven</li>
      <li>Dishwasher</li>
      <li>Cloth Dryer</li>
      <li>Fridge</li>
    </ul>

    <p>We performed cross-tests on different dataset scenarios, each with varying sample sizes:</p>

    <div class="scenario">
      <p><span class="highlight">Scenario A:</span> Training on <span class="highlight">REFIT</span> and testing on <span class="highlight">UK-DALE</span></p>
      <p>Sample Size: 18.3k | Time Window (T) = 256 | Frequency = 60Hz</p>
      <p>Test Set: 3.5k samples</p>
    </div>

    <div class="scenario">
      <p><span class="highlight">Scenario B:</span> Training on <span class="highlight">UK-DALE</span> and testing on <span class="highlight">REFIT</span></p>
      <p>Sample Size: 13.3k</p>
    </div>

    <div class="scenario">
      <p><span class="highlight">Scenario C:</span> Training on <span class="highlight">REFIT</span> and testing on <span class="highlight">REDD</span></p>
      <p>Sample Size: 9.3k</p>
    </div>

    <p>The augmentation pipeline is applied for all scenarios. For training and testing under correlation, we use the corresponding sampling.</p>
  </div>

                <div class="row mt-2">
                    <h3>Dataset</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12">

                        We introduce 3Dident, a dataset with hallmarks of natural environments (shadows, different
                        lighting conditions, 3D rotations, etc.).
                        We publicly released the full dataset (including both, the train and test set) <a href="https://zenodo.org/record/4502485">here</a>.
                        Reference code for evaluation has been made available at <a href="https://github.com/brendel-group/cl-ica">our repository<a/>.

                        <div class="row mt-2">
                            <div class="col-12">
                                <img src="img/3ddis.svg" style="width: 100%;" />
                                <small class="text-muted">
                                    3DIdent: Influence of the latent factors \(\z\) on the renderings \(\x\). Each
                                    column corresponds to a traversal in one of the ten latent dimensions while the
                                    other dimensions are kept fixed.
                                </small>
                            </div>
                        </div>
                    </div>
                </div>


                <div class="row">
                    <h3>Acknowledgements & Funding</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12">
                        <p>
                            This work was granted access to the HPC resources of IDRIS under the allocation AD011014921 made by GENCI (Grand Equipement National de Calcul Intensif). Part of this work was funded by the TotalEnergies Individual Fellowship through One Tech. 
                        </p>
                    </div>
                </div>
                <div class="row">
    <h3>BibTeX</h3>
</div>
<div class="row">
    <p>If you find our analysis helpful, please cite our paper:</p>
</div>
<div class="row justify-content-md-center">
    <div class="col-sm-8 rounded p-3 m-2" style="background-color:lightgray; position:relative;">
        <pre id="bibtexContent" class="code">
@inproceedings{oublal2024disentangling,
  author = {
    Oublal, Khalid and
    Ladjal, Said and
    Benhaiem, David and
    LE BORGNE, Emmanuel and
    Roueff, FranÃ§ois
  },
  title = {
    Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference
  },
  booktitle = {
    The Twelfth International Conference on Learning Representations
  },
  year = {2024},
  url = {
    https://openreview.net/forum?id=iI7hZSczxE
  }
}
        </pre>
        <button onclick="copyBibtex()" style="position:absolute; top:10px; right:10px;">Copy</button>
    </div>
</div>

<script>
    function copyBibtex() {
        const bibtexContent = document.getElementById('bibtexContent').innerText;
        navigator.clipboard.writeText(bibtexContent).then(() => {
            alert('BibTeX citation copied to clipboard!');
        }).catch(err => {
            console.error('Failed to copy text: ', err);
        });
    }
</script>


                <div class="row">
                    <small class="text-muted">Webpage designed using Bootstrap 4.5.</small>
                    <a href="#" class="ml-auto"><i class="fas fa-sort-up"></i></a>
                </div>
                            

            </div>
        </div>

    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>

</html>
